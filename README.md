# Case T√©cnico: Data Mart IDA Anatel

## Vis√£o Geral
Este projeto implementa uma solu√ß√£o de Engenharia de Dados ponta a ponta para a ingest√£o, tratamento e modelagem anal√≠tica dos dados do **√çndice de Desempenho no Atendimento (IDA)** da Anatel. A solu√ß√£o automatiza a extra√ß√£o de arquivos OpenDocument (.ods), normaliza estruturas variadas atrav√©s de processamento Python e consolida as m√©tricas em um Data Mart PostgreSQL seguindo o modelo dimensional (Star Schema).

## Ferramentas e Bibliotecas
- Linguagem: Python 3.11.12
- Banco: PostgreSQL 17.5
- Orquestra√ß√£o: Docker Compose
- Frontend: Streamlit
- Visualiza√ß√£o: Plotly
- Bibliotecas Python:
  - pandas, odfpy (leitura/transforma√ß√£o ODS)
  - psycopg2-binary (PostgreSQL)
  - python-dotenv (configura√ß√£o via ambiente)
  - requests, Playwright (extra√ß√£o opcional)
  - Pillow (tratamento de imagem)
  - matplotlib (apoio visual, opcional)

## Arquitetura da Solu√ß√£o

### 1. Camada de Ingest√£o (Python)
- **ODS Processor**: Motor em Python que utiliza `pandas` e `odfpy` para ler planilhas brutas.
- **Normaliza√ß√£o**: Implementa√ß√£o de l√≥gica robusta para converter tabelas din√¢micas (wide format) em dados normalizados (long format), tratando inconsist√™ncias de cabe√ßalhos e per√≠odos.
- **Bulk Loading**: Persist√™ncia otimizada na Staging via `psycopg2.extras.execute_values`.

### 2. Camada de Dados (PostgreSQL)
- **Staging**: Camada tempor√°ria para persist√™ncia dos dados brutos normalizados.
- **Trusted/Refined (Dimensional)**:
    - `dim_tempo`: Dimens√£o temporal com atributos de calend√°rio.
    - `dim_grupo_economico`: Cadastro √∫nico de operadoras.
    - `dim_servico`: Classifica√ß√£o dos servi√ßos de telecomunica√ß√£o.
    - `fato_ida`: Tabela fato contendo m√©tricas de agilidade (5 dias), resolu√ß√£o total e volumes de solicita√ß√µes.

### 3. Camada Anal√≠tica (SQL)
- **View Pivotada**: Implementa√ß√£o de SQL din√¢mico para gerar relat√≥rios de varia√ß√£o percentual m√™s a m√™s, permitindo compara√ß√£o direta entre o desempenho individual das operadoras e a m√©dia do mercado.
- **Dashboard Interativo**: Interface gr√°fica desenvolvida em **Streamlit** para visualiza√ß√£o amig√°vel dos dados, permitindo an√°lises de tend√™ncia e heatmaps comparativos.

## Execu√ß√£o

A solu√ß√£o √© totalmente conteinerizada via Docker. Siga os passos abaixo:

1. **Pr√©-requisitos**:
   - Docker e Docker Compose instalados.

2. **Subir tudo (um comando)**:
   ```bash
   docker compose up -d
   ```
   - Na primeira execu√ß√£o, o Compose constr√≥i as imagens automaticamente.
   - Ap√≥s remover volumes (reset), os dados s√£o recarregados pelo ETL.

3. **Fluxo Autom√°tico**:
   - O banco PostgreSQL √© inicializado com o schema base.
   - O container `data_loader` aguarda o banco estar `healthy`.
   - Inicia o processamento dos arquivos presentes em `dados_ida/`.
   - Executa as transforma√ß√µes SQL para carga da Fato e cria√ß√£o das Views:
     - [01_transform_load.sql](file:///home/vanessa-aws/projeto_beAnalytic_copia/sql/01_transform_load.sql)
     - [02_view_pivotada.sql](file:///home/vanessa-aws/projeto_beAnalytic_copia/sql/02_view_pivotada.sql)

4. **Ver logs rapidamente**:
   ```bash
   docker compose logs -f data_loader
   ```
   - Aguarde a mensagem: `ETL conclu√≠do com sucesso`.

5. **Reset opcional (apagar dados e subir limpo)**:
   ```bash
   docker compose down -v && docker compose up -d
   ```
   - O ETL recria o Data Mart automaticamente.

## Objetos de Avalia√ß√£o (Prova T√©cnica)
- O projeto roda integralmente com `docker compose up` (infra + ETL + dashboard).
- SQL: organiza√ß√£o, clareza e documenta√ß√£o usando `COMMENT ON`.
- Python: organiza√ß√£o, clareza, docstrings (pydoc) e uso de OOP.
- Sem depend√™ncia de scripts externos; instru√ß√µes m√≠nimas e diretas.

## Estrutura do Projeto
- `/sql`: Scripts de DDL, Transforma√ß√£o e Views.
- `/src`: M√≥dulos Python (Normaliza√ß√£o e Carregamento).
- `/dados_ida`: Reposit√≥rio de arquivos brutos (.ods).
- `docker-compose.yml`: Orquestra√ß√£o da infraestrutura.
- `/assets`: Recursos gr√°ficos do dashboard.

## M√©tricas Principais
- **Taxa de Varia√ß√£o Individual**: Evolu√ß√£o percentual do IDA de uma operadora.
- **Benchmarking de Mercado**: Compara√ß√£o da varia√ß√£o individual contra a m√©dia do setor.
- **M√©trica de Agilidade**: Taxa de resolvidas em at√© 5 dias √∫teis.

## üìú Sequ√™ncia dos Scripts (ETL)
1. Inicializa√ß√£o do schema e tabelas:
   - [00_init_completo.sql](file:///home/vanessa-aws/projeto_beAnalytic_copia/sql/00_init_completo.sql)
2. Transforma√ß√£o e carga para o modelo estrela:
   - [01_transform_load.sql](file:///home/vanessa-aws/projeto_beAnalytic_copia/sql/01_transform_load.sql)
3. Camada anal√≠tica (view com varia√ß√£o e piv√¥):
   - [02_view_pivotada.sql](file:///home/vanessa-aws/projeto_beAnalytic_copia/sql/02_view_pivotada.sql)
4. Orquestra√ß√£o e chamada dos scripts (Python):
   - [carregar_dados.py](file:///home/vanessa-aws/projeto_beAnalytic_copia/carregar_dados.py#L42-L99)

## üß≠ Passo a Passo do ETL
- Ler ODS de `dados_ida/` e normalizar para long-format: [ods_processor.py](file:///home/vanessa-aws/projeto_beAnalytic_copia/src/ods_processor.py)
- Carregar em lote para `ida.staging_ida`: [staging_loader.py](file:///home/vanessa-aws/projeto_beAnalytic_copia/src/staging_loader.py)
- Consolidar dimens√µes e fato: [01_transform_load.sql](file:///home/vanessa-aws/projeto_beAnalytic_copia/sql/01_transform_load.sql)
- Construir view de varia√ß√£o pivoteada: [02_view_pivotada.sql](file:///home/vanessa-aws/projeto_beAnalytic_copia/sql/02_view_pivotada.sql)
- Exibir no dashboard (tema escuro, filtros na lateral, KPIs din√¢micos): [dashboard.py](file:///home/vanessa-aws/projeto_beAnalytic_copia/src/dashboard.py)

## üß™ Valida√ß√µes √öteis
- Contagens r√°pidas (ap√≥s carga):
  ```bash
  docker compose exec postgres psql -U postgres -d ida_datamart -c "SELECT COUNT(*) FROM ida.fato_ida;"
  docker compose exec postgres psql -U postgres -d ida_datamart -c "SELECT COUNT(*) FROM ida.vw_taxa_variacao_pivotada;"
  ```

## üõ†Ô∏è Troubleshooting
- Mensagem ‚ÄúInicializa√ß√£o em andamento‚Äù no dashboard:
  - O ETL ainda est√° criando a view; aguarde ‚ÄúETL completed successfully‚Äù nos logs e recarregue a p√°gina.
- Reconstruir tudo do zero:
  - `docker compose down -v && docker compose up -d`
- Logs do ETL:
  - `docker compose logs -f data_loader`

## ‚úÖ Roadmap de Profissionaliza√ß√£o
- Opcional: integrar dbt para materializar dim/fato/view com testes e documenta√ß√£o.
- Adicionar CI com lint/testes de import (GitHub Actions).

## Transforma√ß√µes com dbt (opcional)
- Projeto dbt inclu√≠do em `/dbt` (models para dim_tempo, dim_grupo_economico, dim_servico e fato_ida).
- Executar dbt via Compose perfil ‚Äúanalytics‚Äù:
  ```bash
  docker compose --profile analytics up dbt
  ```
  ou em execu√ß√£o pontual:
  ```bash
  docker compose run --rm dbt bash -lc "dbt run && dbt test"
  ```
- Conex√£o configurada em `/dbt/profiles.yml` para o Postgres do Compose.
- Modelo anal√≠tico ‚Äúlong_delta‚Äù para varia√ß√£o mensal e diferen√ßa (mercado vs individual).
